{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "060687b6-191c-4436-b895-c68a421bc2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44af2b2c-b112-4bf3-97c6-e0b77ab61783",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='fbref.com', port=443): Max retries exceeded with url: /en/comps/12/2019-2020/2019-2020-La-Liga-Stats (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000027099854C80>: Failed to resolve 'fbref.com' ([Errno 11001] getaddrinfo failed)\"))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:196\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 196\u001b[0m     sock \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mcreate_connection(\n\u001b[0;32m    197\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dns_host, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport),\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout,\n\u001b[0;32m    199\u001b[0m         source_address\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_address,\n\u001b[0;32m    200\u001b[0m         socket_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket_options,\n\u001b[0;32m    201\u001b[0m     )\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\util\\connection.py:60\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LocationParseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, label empty or too long\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgetaddrinfo(host, port, family, socket\u001b[38;5;241m.\u001b[39mSOCK_STREAM):\n\u001b[0;32m     61\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\socket.py:964\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[1;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[0;32m    963\u001b[0m addrlist \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 964\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m _socket\u001b[38;5;241m.\u001b[39mgetaddrinfo(host, port, family, \u001b[38;5;28mtype\u001b[39m, proto, flags):\n\u001b[0;32m    965\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "\u001b[1;31mgaierror\u001b[0m: [Errno 11001] getaddrinfo failed",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mNameResolutionError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    790\u001b[0m     conn,\n\u001b[0;32m    791\u001b[0m     method,\n\u001b[0;32m    792\u001b[0m     url,\n\u001b[0;32m    793\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    794\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    795\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    796\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    797\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    798\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    799\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    800\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    802\u001b[0m )\n\u001b[0;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:490\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    489\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[1;32m--> 490\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[0;32m    492\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 466\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_conn(conn)\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:1095\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1094\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[1;32m-> 1095\u001b[0m     conn\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:615\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    614\u001b[0m sock: socket\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m|\u001b[39m ssl\u001b[38;5;241m.\u001b[39mSSLSocket\n\u001b[1;32m--> 615\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_conn()\n\u001b[0;32m    616\u001b[0m server_hostname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:203\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mNameResolutionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x0000027099854C80>: Failed to resolve 'fbref.com' ([Errno 11001] getaddrinfo failed)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:589\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 589\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    590\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    591\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    592\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[0;32m    593\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    594\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    595\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    596\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    597\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    598\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[0;32m    599\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    600\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    601\u001b[0m     )\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:843\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    841\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[1;32m--> 843\u001b[0m retries \u001b[38;5;241m=\u001b[39m retries\u001b[38;5;241m.\u001b[39mincrement(\n\u001b[0;32m    844\u001b[0m     method, url, error\u001b[38;5;241m=\u001b[39mnew_e, _pool\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, _stacktrace\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m    845\u001b[0m )\n\u001b[0;32m    846\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\util\\retry.py:519\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    518\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[1;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    521\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='fbref.com', port=443): Max retries exceeded with url: /en/comps/12/2019-2020/2019-2020-La-Liga-Stats (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000027099854C80>: Failed to resolve 'fbref.com' ([Errno 11001] getaddrinfo failed)\"))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Fetch the page\u001b[39;00m\n\u001b[0;32m      6\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://fbref.com/en/comps/12/2019-2020/2019-2020-La-Liga-Stats\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Replace with the actual URL of the page you want to scrape\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Check if the request was successful\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:622\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    618\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[0;32m    619\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[0;32m    620\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m--> 622\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    624\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    625\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='fbref.com', port=443): Max retries exceeded with url: /en/comps/12/2019-2020/2019-2020-La-Liga-Stats (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000027099854C80>: Failed to resolve 'fbref.com' ([Errno 11001] getaddrinfo failed)\"))"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd  # Import pandas\n",
    "\n",
    "# Fetch the page\n",
    "url = 'https://fbref.com/en/comps/12/2019-2020/2019-2020-La-Liga-Stats'  # Replace with the actual URL of the page you want to scrape\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find the table with the specified ID\n",
    "    table = soup.find('table', {'id': 'results2019-2020121_overall'})\n",
    "    \n",
    "    # Initialize a list to collect team data\n",
    "    data = []\n",
    "\n",
    "    # Loop through each row in the table body\n",
    "    for row in table.tbody.find_all('tr'):\n",
    "        team_data = {\n",
    "            'Rank': row.find('th', {'data-stat': 'rank'}).get_text(strip=True),  # Get the rank\n",
    "            'Team': row.find('td', {'data-stat': 'team'}).find('a').get_text(strip=True),  # Extract the team name\n",
    "            'Games': row.find('td', {'data-stat': 'games'}).get_text(strip=True),  # Get the number of games played\n",
    "            'Wins': row.find('td', {'data-stat': 'wins'}).get_text(strip=True),  # Get the number of wins\n",
    "            'Ties': row.find('td', {'data-stat': 'ties'}).get_text(strip=True),  # Get the number of ties\n",
    "            'Losses': row.find('td', {'data-stat': 'losses'}).get_text(strip=True),  # Get the number of losses\n",
    "            'Goals For': row.find('td', {'data-stat': 'goals_for'}).get_text(strip=True),  # Get goals scored\n",
    "            'Goals Against': row.find('td', {'data-stat': 'goals_against'}).get_text(strip=True),  # Get goals conceded\n",
    "            'Goal Difference': row.find('td', {'data-stat': 'goal_diff'}).get_text(strip=True),  # Get goal difference\n",
    "            'Points': row.find('td', {'data-stat': 'points'}).get_text(strip=True),  # Get points\n",
    "           \n",
    "\n",
    "        }\n",
    "        data.append(team_data)  # Append each dictionary to the data list\n",
    "\n",
    "    # Check if any data was collected\n",
    "    if data:\n",
    "        # Create a DataFrame from the list of dictionaries\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        # Display the DataFrame\n",
    "        print(df)\n",
    "\n",
    "    else:\n",
    "        print(\"No data was collected.\")\n",
    "\n",
    "else:\n",
    "    print(f\"Failed to retrieve page: {response.status_code}\")\n",
    "    \n",
    "#THIS EXTRACTS THE SCRAPED DATA TO A CSV FILE\n",
    "df.to_csv(r'C:\\Users\\kimfa\\Documents\\laliga\\stats4.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7804a5-83ad-472b-a6cd-56d6284625d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#THIS IS A REPETITIN OF THE PREVIOUS CODE TO RETRIEVE DATA FROM THE SAME WEBSITE BUT FOR A DIFFERENT SEASON , WHICH IS WHY IT HAS LESS COMMENTS\n",
    "url = 'https://fbref.com/en/comps/12/2020-2021/2020-2021-La-Liga-Stats'\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup=BeautifulSoup(response.content, 'html.parser')\n",
    "    table=soup.find('table',{'id': 'results2020-2021121_overall'})\n",
    "    data = []\n",
    "    for row in table.tbody.find_all('tr'):\n",
    "        team_data= {\n",
    "             'Rank': row.find('th', {'data-stat': 'rank'}).get_text(strip=True),  # Get the rank\n",
    "            'Team': row.find('td', {'data-stat': 'team'}).find('a').get_text(strip=True),  # Extract the team name\n",
    "            'Games': row.find('td', {'data-stat': 'games'}).get_text(strip=True),  # Get the number of games played\n",
    "            'Wins': row.find('td', {'data-stat': 'wins'}).get_text(strip=True),  # Get the number of wins\n",
    "            'Ties': row.find('td', {'data-stat': 'ties'}).get_text(strip=True),  # Get the number of ties\n",
    "            'Losses': row.find('td', {'data-stat': 'losses'}).get_text(strip=True),  # Get the number of losses\n",
    "            'Goals For': row.find('td', {'data-stat': 'goals_for'}).get_text(strip=True),  # Get goals scored\n",
    "            'Goals Against': row.find('td', {'data-stat': 'goals_against'}).get_text(strip=True),  # Get goals conceded\n",
    "            'Goal Difference': row.find('td', {'data-stat': 'goal_diff'}).get_text(strip=True),  # Get goal difference\n",
    "            'Points': row.find('td', {'data-stat': 'points'}).get_text(strip=True),  # Get points\n",
    "        }\n",
    "        data.append(team_data)\n",
    "    if data:\n",
    "        df=pd.DataFrame(data)\n",
    "        print(df) \n",
    "\n",
    "#THIS EXTRACTS THE SCRAPED DATA TO A CSV FILE\n",
    "df.to_csv(r'C:\\Users\\kimfa\\Documents\\laliga\\stats2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3470f72-81d6-40c6-a902-1e9aae73f2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#THIS IS A REPETITON OF THE PREVIOUS CODE TO RETRIEVE DATA FROM THE SAME WEBSITE BUT FOR A DIFFERENT SEASON , WHICH IS WHY IT HAS LESS COMMENTS\n",
    "url = 'https://fbref.com/en/comps/12/2021-2022/2021-2022-La-Liga-Stats'\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup=BeautifulSoup(response.content, 'html.parser')\n",
    "    table=soup.find('table',{'id': 'results2021-2022121_overall'})\n",
    "    data = []\n",
    "    for row in table.tbody.find_all('tr'):\n",
    "        team_data= {\n",
    "             'Rank': row.find('th', {'data-stat': 'rank'}).get_text(strip=True),  # Get the rank\n",
    "            'Team': row.find('td', {'data-stat': 'team'}).find('a').get_text(strip=True),  # Extract the team name\n",
    "            'Games': row.find('td', {'data-stat': 'games'}).get_text(strip=True),  # Get the number of games played\n",
    "            'Wins': row.find('td', {'data-stat': 'wins'}).get_text(strip=True),  # Get the number of wins\n",
    "            'Ties': row.find('td', {'data-stat': 'ties'}).get_text(strip=True),  # Get the number of ties\n",
    "            'Losses': row.find('td', {'data-stat': 'losses'}).get_text(strip=True),  # Get the number of losses\n",
    "            'Goals For': row.find('td', {'data-stat': 'goals_for'}).get_text(strip=True),  # Get goals scored\n",
    "            'Goals Against': row.find('td', {'data-stat': 'goals_against'}).get_text(strip=True),  # Get goals conceded\n",
    "            'Goal Difference': row.find('td', {'data-stat': 'goal_diff'}).get_text(strip=True),  # Get goal difference\n",
    "            'Points': row.find('td', {'data-stat': 'points'}).get_text(strip=True),  # Get points\n",
    "        }\n",
    "        data.append(team_data)\n",
    "    if data:\n",
    "        df=pd.DataFrame(data)\n",
    "        print(df)\n",
    "        \n",
    "#THIS EXTRACTS THE SCRAPED DATA TO A CSV FILE\n",
    "df.to_csv(r'C:\\Users\\kimfa\\Documents\\laliga\\stats3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c4a9da-a621-421c-b106-f407aa7b821c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#THIS IS A REPETITON OF THE PREVIOUS CODE TO RETRIEVE DATA FROM THE SAME WEBSITE BUT FOR A DIFFERENT SEASON , WHICH IS WHY IT HAS LESS COMMENTS\n",
    "url = 'https://fbref.com/en/comps/12/2022-2023/2022-2023-La-Liga-Stats'\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup=BeautifulSoup(response.content, 'html.parser')\n",
    "    table=soup.find('table',{'id': 'results2022-2023121_overall'})\n",
    "    data = []\n",
    "    for row in table.tbody.find_all('tr'):\n",
    "        team_data= {\n",
    "             'Rank': row.find('th', {'data-stat': 'rank'}).get_text(strip=True),  # Get the rank\n",
    "            'Team': row.find('td', {'data-stat': 'team'}).find('a').get_text(strip=True),  # Extract the team name\n",
    "            'Games': row.find('td', {'data-stat': 'games'}).get_text(strip=True),  # Get the number of games played\n",
    "            'Wins': row.find('td', {'data-stat': 'wins'}).get_text(strip=True),  # Get the number of wins\n",
    "            'Ties': row.find('td', {'data-stat': 'ties'}).get_text(strip=True),  # Get the number of ties\n",
    "            'Losses': row.find('td', {'data-stat': 'losses'}).get_text(strip=True),  # Get the number of losses\n",
    "            'Goals For': row.find('td', {'data-stat': 'goals_for'}).get_text(strip=True),  # Get goals scored\n",
    "            'Goals Against': row.find('td', {'data-stat': 'goals_against'}).get_text(strip=True),  # Get goals conceded\n",
    "            'Goal Difference': row.find('td', {'data-stat': 'goal_diff'}).get_text(strip=True),  # Get goal difference\n",
    "            'Points': row.find('td', {'data-stat': 'points'}).get_text(strip=True),  # Get points\n",
    "        }\n",
    "        data.append(team_data)\n",
    "    if data:\n",
    "        df=pd.DataFrame(data)\n",
    "        print(df)\n",
    "\n",
    "#THIS EXTRACTS THE SCRAPED DATA TO A CSV FILE\n",
    "df.to_csv(r'C:\\Users\\kimfa\\Documents\\laliga\\stats4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5430d245-b946-4fa2-9780-88a144401136",
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS WILL MERGE ALL DATA INTO A SINGLE CSV\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Path to a folder containing the CSV files\n",
    "folder_path = r\"C:\\Users\\kimfa\\Documents\\laliga\\*.csv\"  # Adjust to the correct path\n",
    "\n",
    "# Use glob to list all CSV files in the folder\n",
    "file_paths = glob.glob(folder_path)\n",
    "\n",
    "# Check if any files were found\n",
    "if not file_paths:\n",
    "    print(\"No CSV files found in the specified folder.\")\n",
    "else:\n",
    "    # Initialize a list to store each DataFrame\n",
    "    df_list = []\n",
    "\n",
    "    # Loop through each file, read the data, and add a 'Year' column based on the filename\n",
    "    for file in file_paths:\n",
    "        year = file.split('_')[-1].split('.')[0]  # Adjust based on file naming pattern\n",
    "        df = pd.read_csv(file)\n",
    "        df['Year'] = year  # Add year column to each DataFrame\n",
    "        df_list.append(df)\n",
    "\n",
    "    # Concatenate all DataFrames into a single DataFrame\n",
    "    merged_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    # Save the merged DataFrame to a new CSV file\n",
    "    merged_df.to_csv(\"merged_laliga_standings.csv\", index=False)\n",
    "\n",
    "    # Optionally, print to verify\n",
    "    print(merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1becb14b-43a2-4edc-bd40-d73b214d6233",
   "metadata": {},
   "outputs": [],
   "source": [
    "#THE FILE WAS CLEANED,TRIMMED AND ADJUSTED IN EXCEL BEFORE IT WAS REUPLOADED TO JUPYTER NOTEBOOK FOR FURTHER ANALYSIS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
